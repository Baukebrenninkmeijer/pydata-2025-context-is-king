---
marp: true
title: Context is King
html: true
format:
  revealjs:
    theme: [Sketchy.scss, styles.scss]
    footer: "Context is King â€¢ Bauke Brenninkmeijer"
    width: 1280
    height: 720
    toc: true
    toc-depth: 1
    show-slide-number: speaker
    title-slide-attributes: 
      data-background-color: transparent
      class: hide-slide
---

```{=html}
<div style="text-align: center;">
  <img src="img/moire.svg" alt="Moire pattern" style="max-width: 80%; height: auto;">
  <h1 style="margin-top: 2rem;">Context is King</h1>
</div>
```

---

## {visibility="hidden"}

## Research question:
- When must we switch from context window retrieval to RAG
- Can we skip RAG and immediately work with full context window when sources fit in the context window? 
- Is there a context window threshold where we observe a sharp dropoff in performance?
- Does reranking still make sense in a post-RAG world? 
- If small context is still needed, and reranking does not help, how can we best utilize the effective context window?
- With growing context windows, how does the performance of RAG change when we start to include more retrieved chunks?
- Do models with larger context windows outperform models with smaller context windows at the same context window size. For example, does a 1M token context window perform better than a 100k token context window at the 100k token context window size?
- Chroma research indicates that shuffling data improves retrieval performance within the context window. If this is the case, should we start shuffling data for context-window retrieval?

### Cost-performance trade-off
- At what cost multiplier does RAG become preferable regardless of performance?
- Does cost-performance trade-off change with context window size?

### Latency implications
- How does latency change with context window size?
- What are the latency thresholds where RAG becomes necessary for user experience?

### Dynamic context management: (optional)
- How should systems adaptively choose between RAG and long context based on query characteristics?
- Can we develop hybrid approaches that use long context for certain queries, and RAG for others?

### Hybrid Approach Optimization (out of scope)

- What are the optimal hybrid strategies that combine RAG with selective long context usage?
- How should we partition information between immediate context and retrieval corpus in hybrid systems?



## Hypothesis

1.  Long context allows us to skip RAG for certain situations that easily fit into context window
2. There is some context windows threshold where we observe a sharp dropoff in performance.
3. Reranking does not hold value for long context windows, except when using reranker thresholds as filtering mechanism.
4. Cost-adjusted performance favors RAG even when long context technically performs better, with the crossover point occurring at [X]% context window utilization.
5. Hybrid approaches that strategically combine RAG with selective long context usage outperform pure strategies across most practical scenarios.
6. The optimal context strategy is predictable from document corpus characteristics, enabling automated strategy selection.

## Experiment setup
We have results from context window retrieval. We will re-use the same datasets as chroma has done in their research, with common best practices. 

## Notable points
- Llama has a 10M model, but it's only served up to 128k. Together AI serves Scout up to 1M (10% of max), openrouter also 1M max.
- Google reduced the max context size window from 2M back to 1M for Gemini 2.5. What does this tell us?


## Evaluation Metrics
- Beyond accuracy, include:
  - Cost per query (normalized across approaches)
  - End-to-end latency
  - Memory utilization


---
